{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Shri.DESKTOP-3RITHQH\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shri.DESKTOP-3RITHQH\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Shri.DESKTOP-3RITHQH\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import chardet\n",
    "\n",
    "# Download required packages\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('cmudict')\n",
    "\n",
    "# Path to the text files\n",
    "path = r\"C:\\Users\\Shri.DESKTOP-3RITHQH\\Data Science\"\n",
    "\n",
    "# List of text files\n",
    "text_files = [f for f in os.listdir(path) if f.endswith('.txt')]\n",
    "\n",
    "# Loop through the text files\n",
    "for file in text_files[:150]:\n",
    "    # Read the contents of the file\n",
    "    def read_file(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            result = chardet.detect(f.read())\n",
    "            f.seek(0)\n",
    "            text = f.read().decode(result['encoding'])\n",
    "        return text\n",
    "\n",
    "    text = read_file(os.path.join(path, file))\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    positive_score = sentiment['pos']\n",
    "    negative_score = sentiment['neg']\n",
    "    polarity_score = sentiment['compound']\n",
    "\n",
    "    # Text Statistics\n",
    "    tokens = word_tokenize(text)\n",
    "    total_word_count = len(tokens)\n",
    "    syllables_per_word = 0\n",
    "    complex_word_count = 0\n",
    "    personal_pronouns = 0\n",
    "    avg_word_length = 0\n",
    "    for word in tokens:\n",
    "        if word.lower() in nltk.corpus.cmudict.dict():\n",
    "            syllables = nltk.corpus.cmudict.dict()[word.lower()][0]\n",
    "            syllables_per_word += len([syl for syl in syllables if syl[-1].isdigit()])\n",
    "        if len(word) >= 6:\n",
    "            complex_word_count += 1\n",
    "        avg_word_length += len(word)\n",
    "        if word.lower() in ['he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'they', 'them', 'their', 'theirs', 'themselves', 'i', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself', 'we', 'us', 'our', 'ours', 'ourselves']:\n",
    "            personal_pronouns += 1\n",
    "    avg_word_length /= total_word_count\n",
    "\n",
    "    # Compute average sentence length and fog index\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentence_count = len(sentences)\n",
    "    avg_sentence_length = total_word_count / sentence_count\n",
    "    fog_index = 0.4 * (avg_sentence_length + 100 * (complex_word_count / total_word_count))\n",
    "    # Output the results\n",
    "    print(\"File:\", file)\n",
    "print(\"Positive Score:\", positive_score)\n",
    "print(\"Negative Score:\", negative_score)\n",
    "print(\"Polarity Score:\", polarity_score)\n",
    "print(\"Average Sentence Length:\", avg_sentence_length)\n",
    "print(\"Percentage of Complex Words:\", 100 * (complex_word_count / total_word_count))\n",
    "print(\"FOG Index:\", fog_index)\n",
    "print(\"Complex Word Count:\", complex_word_count)\n",
    "print(\"Word Count:\", total_word_count)\n",
    "print(\"Syllables per Word:\", syllables_per_word)\n",
    "print(\"Personal Pronouns:\", personal_pronouns)\n",
    "print(\"Average Word Length:\", avg_word_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Define the path to the text files\n",
    "path = \"C:\\Users\\Shri.DESKTOP-3RITHQH\\Data Science\"\n",
    "\n",
    "# Load the SentimentIntensityAnalyzer for sentiment analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Iterate over the text files\n",
    "for filename in os.listdir(path):\n",
    "    # Open the file\n",
    "    with open(os.path.join(path, filename), 'r') as file:\n",
    "        # Read the text from the file\n",
    "        text = file.read()\n",
    "        \n",
    "        # Tokenize the text into sentences and words\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        words = [word.lower() for sent in sentences for word in word_tokenize(sent) if word not in string.punctuation]\n",
    "        \n",
    "        # Lemmatize the words\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "        # Compute the sentiment scores for each sentence\n",
    "        sentiment_scores = [sia.polarity_scores(sent) for sent in sentences]\n",
    "        \n",
    "        # Compute the positive score\n",
    "        positive_score = sum(score['pos'] for score in sentiment_scores) / len(sentiment_scores)\n",
    "        \n",
    "        # Compute the negative score\n",
    "        negative_score = sum(score['neg'] for score in sentiment_scores) / len(sentiment_scores)\n",
    "        \n",
    "        # Compute the polarity score\n",
    "        polarity_score = sum(score['compound'] for score in sentiment_scores) / len(sentiment_scores)\n",
    "        \n",
    "        # Compute the subjectivity score\n",
    "        subjectivity_score = sum(score['compound'] for score in sentiment_scores) / len(sentiment_scores)\n",
    "        \n",
    "        # Compute the average sentence length\n",
    "        avg_sentence_length = sum(len(word_tokenize(sent)) for sent in sentences) / len(sentences)\n",
    "        \n",
    "        # Compute the percentage of complex words\n",
    "        num_complex_words = 0\n",
    "        for word in words:\n",
    "            synsets = wordnet.synsets(word)\n",
    "            if len(synsets) > 1:\n",
    "                num_complex_words += 1\n",
    "        percentage_complex_words = num_complex_words / len(words)\n",
    "        \n",
    "        # Compute the fog index\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "        \n",
    "        # Compute the average number of words per sentence\n",
    "        avg_num_words_per_sentence = len(words) / len(sentences)\n",
    "        \n",
    "        # Compute the complex word count\n",
    "        complex_word_count = num_complex_words\n",
    "        # Compute the word count\n",
    "    word_count = len(words)\n",
    "    \n",
    "    # Compute the syllables per word\n",
    "    def count_syllables(word):\n",
    "        syllables = 0\n",
    "        for v in word:\n",
    "            if v in \"aeiouyAEIOUY\":\n",
    "                syllables += 1\n",
    "        if syllables == 0:\n",
    "            syllables += 1\n",
    "        return syllables\n",
    "    \n",
    "    syllables_per_word = sum(count_syllables(word) for word in words) / len(words)\n",
    "    \n",
    "    # Compute the number of personal pronouns\n",
    "    personal_pronouns = ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'they', 'them', 'their', 'theirs']\n",
    "    num_personal_pronouns = sum(1 for word in words if word in personal_pronouns)\n",
    "    \n",
    "    # Compute the average word length\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    # Output the results\n",
    "    print('Filename:', filename)\n",
    "    print('Positive Score:', positive_score)\n",
    "    print('Negative Score:', negative_score)\n",
    "    print('Polarity Score:', polarity_score)\n",
    "    print('Subjectivity Score:', subjectivity_score)\n",
    "    print('Avg Sentence Length:', avg_sentence_length)\n",
    "    print('Percentage of Complex Words:', percentage_complex_words)\n",
    "    print('Fog Index:', fog_index)\n",
    "    print('Avg Number of Words per Sentence:', avg_num_words_per_sentence)\n",
    "    print('Complex Word Count:', complex_word_count)\n",
    "    print('Word Count:', word_count)\n",
    "    print('Syllables per Word:', syllables_per_word)\n",
    "    print('Personal Pronouns:', num_personal_pronouns)\n",
    "    print('Avg Word Length:', avg_word_length)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
